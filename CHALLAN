# GIVE YOUR OWN ORACLE CREDENTIAL OR .env file. Watch a video how & where to save .env credential file.

import pandas as pd
from sqlalchemy import create_engine
import cx_Oracle
import logging
import os
from datetime import datetime
from dotenv import load_dotenv


os.environ["PATH"] = r"C:\oracle\instantclient;" + os.environ["PATH"]

# Initialize Oracle Client
cx_Oracle.init_oracle_client(lib_dir=r"C:\oracle\instantclient")

# Logging setup
logging.basicConfig(level=logging.INFO)

# Load .env file from the current directory
dotenv_path = os.path.join(os.getcwd(), 'etl_pass_security.env')
load_dotenv(dotenv_path)

# Get the values
username = os.getenv('ORACLE_USERNAME')
password = os.getenv('ORACLE_PASSWORD')
host = os.getenv('ORACLE_HOST')
port = os.getenv('ORACLE_PORT')
service_name = os.getenv('ORACLE_SERVICE_NAME')

# Check if all variables are loaded correctly
# print("Username:", username)
# print("Host:", host)
# print("Service:", service_name)


# Create DSN and engine
dsn = cx_Oracle.makedsn(host, port, service_name=service_name)
engine = create_engine(f'oracle+cx_oracle://{username}:{password}@{dsn}')


# Column definitions
CHALLAN_MASTER_COLUMNS = (
    "challan_no, challan_trn, company_id, ch_date, dist_id, wh_id, trip_no, confirm, note, iuser, "
    "euser, idate, edate, ver, ref_no, cancelled, oid, chma_acyl, udt, chma_ackn, site, chma_seqn, delv_note "
)

CHALLAN_DETAIL_COLUMNS = (
    "challan_no, do_line_no, item_id, challan_line_no, rate, qty, e_rate, e_qty, s_qty, iuser, euser, "
    "idate, edate, do_no, cancelled, ver, ch_date, pctdisc, oid, chde_acyl, udt, chde_lubl, d_qty, period, "
    "chde_lcbl, chde_ruid, chde_duid, chde_dufq, chde_fcur, chde_fcrt, site, chde_seqn, line_note, "
    "line_note2, seqn, n_oid, company_id"
)

# Extract a single batch
def extract_batch(table_name, start_date, end_date, offset, batch_size):
    if table_name == 'CHALLAN_MASTER':
        columns = CHALLAN_MASTER_COLUMNS
    elif table_name == 'CHALLAN_DETAIL':
        columns = CHALLAN_DETAIL_COLUMNS
    else:
        logging.error(f"Unsupported table name: {table_name}")
        return pd.DataFrame()

    sql_query = f"""
    WITH paged_data AS (
        SELECT {columns}, ROW_NUMBER() OVER (ORDER BY CH_DATE) AS row_num
        FROM {table_name}
        WHERE CH_DATE BETWEEN TO_DATE('{start_date}', 'DD-MM-YYYY')
        AND TO_DATE('{end_date}', 'DD-MM-YYYY')
    )
    SELECT {columns}
    FROM paged_data
    WHERE row_num > {offset} AND row_num <= {offset + batch_size}
    """

    logging.info(f"Extracting {table_name}: Offset {offset}")
    try:
        df = pd.read_sql(sql_query, con=engine)
        return df
    except Exception as e:
        logging.error(f"Error extracting data: {e}")
        return pd.DataFrame()



def transform_challan_detail(df):
    """
    Transform the CHALLAN_DETAIL data, removing rows with invalid data types.
    """
    # Convert date columns to datetime and filter out invalid rows
    date_cols = ['idate', 'edate', 'ch_date']
    for col in date_cols:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], errors='coerce')
    df = df.dropna(subset=date_cols) # Remove rows where date conversion failed

    # Clean and transform numeric columns, remove rows with invalid numeric values
    numeric_cols = ['rate', 'qty', 'e_rate', 'e_qty', 's_qty', 'pctdisc', 'd_qty']
    for col in numeric_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
    df = df.dropna(subset=numeric_cols)  # Remove rows where numeric conversion failed

    # Clean text columns
    text_cols = ['challan_no', 'item_id', 'do_no', 'line_note', 'line_note2']
    for col in text_cols:
        if col in df.columns:
            df[col] = df[col].astype(str).str.strip()
    
    # Handle NULL values
    if 'chde_lcbl' in df.columns:
        df['chde_lcbl'] = df['chde_lcbl'].replace('NULL', None)
    if 'n_oid' in df.columns:
        df['n_oid'] = df['n_oid'].replace('NULL', None)
    
    
    
    # Calculate extended amounts
    if 'rate' in df.columns and 'qty' in df.columns:
        df['total_amount'] = df['rate'] * df['qty']
        if 'pctdisc' in df.columns:
            df['discounted_amount'] = df['total_amount'] * (1 - (df['pctdisc'] / 100))  
        # if 'pctdise' in df.columns:
        #     df['discount_amount'] = df['total_amount'] - df['discounted_amount']    
        if 'pctdisc' in df.columns and 'discounted_amount' in df.columns:
            df['discount_amount'] = df['total_amount'] - df['discounted_amount']

    # Create status flags
    # if 'iuser' in df.columns and 'euser' in df.columns:
    #     df['is_edited'] = df['iuser'] != df['euser']
    
    # Clean user IDs
    if 'iuser' in df.columns:
        df['iuser'] = df['iuser'].astype(str).str.strip()
    if 'euser' in df.columns:
        df['euser'] = df['euser'].astype(str).str.strip()
    
    return df


def transform_challan_master(df):
    """
    Transform the CHALLAN_MASTER data, removing rows with invalid data types.
    """
    # Convert date columns to datetime and filter out invalid rows
    date_cols = ['ch_date', 'idate', 'edate']
    for col in date_cols:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], errors='coerce')
    df = df.dropna(subset=date_cols)  # Remove rows where date conversion failed        
    
    # Clean text columns
    text_cols = ['challan_no', 'challan_trn', 'company_id', 'note', 'delv_note']
    for col in text_cols:
        if col in df.columns:
            df[col] = df[col].astype(str).str.strip()
    
    # Handle boolean columns
    if 'confirm' in df.columns:
        df['confirm'] = df['confirm'].map({'Y': True, 'N': False})
    if 'cancelled' in df.columns:
        df['cancelled'] = df['cancelled'] == 'Y'

    
    # Clean user IDs
    if 'iuser' in df.columns:
        df['iuser'] = df['iuser'].astype(str).str.strip()
    if 'euser' in df.columns:
        df['euser'] = df['euser'].astype(str).str.strip()
    
    # # Create status flags
    # if 'iuser' in df.columns and 'euser' in df.columns:
    #     df['is_edited'] = df['iuser'] != df['euser']
    
    return df

def join_challan_data(detail_df, master_df):
   
    #Join the detail and master data with common transformations.
    #Automatically merges columns with duplicate values (e.g., detail_ch_date == master_ch_date).
    
    # Standardize column names before join
    detail_df = detail_df.rename(columns={
        'ch_date': 'detail_ch_date',
        'idate': 'detail_idate',
        'edate': 'detail_edate'
    })
    
    master_df = master_df.rename(columns={
        'ch_date': 'master_ch_date',
        'idate': 'master_idate',
        'edate': 'master_edate'
    })
    
    # Perform the join
    merged_df = pd.merge(
        detail_df,
        master_df,
        on='challan_no',
        how='left',
        suffixes=('_detail', '_master')
    )

    # Automatically merge columns where detail_* and master_* values are the same
    for detail_col in merged_df.columns:
        if detail_col.startswith('detail_'):
            base_col = detail_col.replace('detail_', '')
            master_col = f'master_{base_col}'
            new_col_name = base_col

            if master_col in merged_df.columns:
                merged_df = merge_duplicate_columns(merged_df, detail_col, master_col, new_col_name)

    return merged_df

def merge_duplicate_columns(df, col1, col2, new_col_name):
    """
    If col1 and col2 have the same values in all rows, merge into new_col_name and drop originals.
    """
    if col1 in df.columns and col2 in df.columns:
        if df[col1].equals(df[col2]):
            df[new_col_name] = df[col1]
            df.drop([col1, col2], axis=1, inplace=True)
            logging.info(f"Merged columns '{col1}' and '{col2}' into '{new_col_name}'")
        else:
            logging.info(f"Columns '{col1}' and '{col2}' have different values â€” not merging.")
    return df
   

# Save final CSVs
def save_final_csvs(master_df, detail_df, merged_df):
    output_dir = r"D:/ETL DEMO WORK/DEMO"
    os.makedirs(output_dir, exist_ok=True)
    
    try:
        # Save individual files
        master_path = os.path.join(output_dir, "CHALLAN_MASTER.csv")
        master_df.to_csv(master_path, index=False)
        logging.info(f"Master data saved: {master_path}")
        
        detail_path = os.path.join(output_dir, "CHALLAN_DETAIL.csv")
        detail_df.to_csv(detail_path, index=False)
        logging.info(f"Detail data saved: {detail_path}")
        

        # Customize column order for merged_df(MERGED_CHALLAN_DATA) and then  # Save MERGED_CHALLAN_DATA
        desired_column_order = [
            'challan_no',
            'item_id',
            'do_no',
            'dist_id',
            'rate',
            'qty',
            'pctdisc',
            'total_amount',
            'discounted_amount',
            'discount_amount',
            'master_ch_date',
            'master_idate',
            'master_edate',
            'line_note',
            'line_note2',
            'detail_ch_date',
            'detail_idate',
            'detail_edate',
            'confirm',
            'cancelled',
            'iuser',
            'euser'
        ]

        # Reorder columns safely: only use columns that exist
        existing_columns = [col for col in desired_column_order if col in merged_df.columns]
        remaining_columns = [col for col in merged_df.columns if col not in existing_columns]
        final_columns = existing_columns + remaining_columns

        merged_df = merged_df[final_columns]

        # Save merged file
        merged_path = os.path.join(output_dir, "MERGED_CHALLAN_DATA.csv")
        merged_df.to_csv(merged_path, index=False)
        logging.info(f"Merged data saved: {merged_path}")
        
    except Exception as e:
        logging.error(f"Error saving CSV files: {e}")

# Function to get total record count
def get_total_records(table_name, start_date, end_date):
    sql_query = f"""
    SELECT COUNT(*) 
    FROM {table_name}
    WHERE CH_DATE BETWEEN TO_DATE('{start_date}', 'DD-MM-YYYY')
    AND TO_DATE('{end_date}', 'DD-MM-YYYY')
    """
    try:
        total_records = pd.read_sql(sql_query, con=engine).iloc[0, 0]
        return total_records
    except Exception as e:
        logging.error(f"Error fetching total records: {e}")
        return 0

# Main ETL runner
def run_etl():
    start_date = '01-01-2023'
    end_date = '31-01-2023'
    batch_size = 10000
    
    # Process CHALLAN_MASTER
    master_total = get_total_records('CHALLAN_MASTER', start_date, end_date)
    master_batches = []
    for offset in range(0, master_total, batch_size):
        batch_df = extract_batch('CHALLAN_MASTER', start_date, end_date, offset, batch_size)
        if not batch_df.empty:
            transformed = transform_challan_master(batch_df)
            master_batches.append(transformed)
    
    master_df = pd.concat(master_batches, ignore_index=True) if master_batches else pd.DataFrame()
    
    # Process CHALLAN_DETAIL
    detail_total = get_total_records('CHALLAN_DETAIL', start_date, end_date)
    detail_batches = []
    for offset in range(0, detail_total, batch_size):
        batch_df = extract_batch('CHALLAN_DETAIL', start_date, end_date, offset, batch_size)
        if not batch_df.empty:
            transformed = transform_challan_detail(batch_df)
            detail_batches.append(transformed)
    
    detail_df = pd.concat(detail_batches, ignore_index=True) if detail_batches else pd.DataFrame()
    
    # Join data if both DataFrames are not empty
    if not master_df.empty and not detail_df.empty:
        merged_df = join_challan_data(detail_df, master_df)
    else:
        merged_df = pd.DataFrame()
        logging.warning("Could not merge data - one or both DataFrames are empty")

    # Save all files
    save_final_csvs(master_df, detail_df, merged_df)

# Run the ETL process
if __name__ == "__main__":
    run_etl()
